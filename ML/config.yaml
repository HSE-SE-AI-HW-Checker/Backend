# Конфигурация LLaMA Local

# Настройки загрузки модели
download:
  # ID репозитория на Hugging Face
  repo_id: "bartowski/Meta-Llama-3.1-8B-Instruct-GGUF"
  
  # Имя файла модели для загрузки
  filename: "Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"
  
  # Автоматическая загрузка модели, если она отсутствует
  auto_download: true
  
  # Токен Hugging Face (опционально, для приватных моделей)
  # Можно также установить через переменную окружения HF_TOKEN
  token: "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"

# Настройки модели
model:
  # Путь к файлу модели (GGUF формат)
  # Примеры:
  # - ./models/llama-3.2-1b-instruct-q4_k_m.gguf
  # - ./models/llama-3.2-3b-instruct-q4_k_m.gguf
  path: "./models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"
  
  # Размер контекста (количество токенов)
  n_ctx: 8192
  
  # Количество слоев для GPU offload
  # -1 = все слои на GPU (рекомендуется для Apple Silicon с Metal)
  # 0 = только CPU
  # >0 = указанное количество слоев на GPU
  n_gpu_layers: -1
  
  # Параметры генерации
  temperature: 0.7      # Креативность (0.0-2.0, выше = более креативно)
  top_p: 0.9           # Nucleus sampling (0.0-1.0)
  top_k: 40            # Top-K sampling
  repeat_penalty: 1.1  # Штраф за повторения (1.0 = нет штрафа)
  max_tokens: 512      # Максимальное количество токенов в ответе
  stream: true         # Streaming вывод токенов

# Настройки приложения
app:
  # Уровень логирования: DEBUG, INFO, WARNING, ERROR, CRITICAL
  log_level: "INFO"